{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/kaggle/working\n"
     ]
    }
   ],
   "source": [
    "!rm -r /kaggle/working/*\n",
    "%cd /kaggle/working"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "\n",
    "PACKAGE_DIR = \"/kaggle/src\"\n",
    "sys.path.append(PACKAGE_DIR)\n",
    "sys.path.append(os.path.join(PACKAGE_DIR, \"Penguin-ML-Library\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-11-21 10:10:29.057830: I tensorflow/core/util/port.cc:113] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2024-11-21 10:10:29.091398: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "exp_018\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/pty.py:89: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n",
      "  pid, fd = os.forkpty()\n",
      "set seed: 46\n"
     ]
    }
   ],
   "source": [
    "import yaml\n",
    "from penguinml.utils.logger import get_logger, init_logger\n",
    "from penguinml.utils.set_seed import seed_base_torch\n",
    "\n",
    "MODEL_NAME = \"deberta\"\n",
    "CFG = yaml.safe_load(open(os.path.join(PACKAGE_DIR, \"config.yaml\"), \"r\"))\n",
    "print(CFG[MODEL_NAME][\"execution\"][\"exp_id\"])\n",
    "CFG[\"output_dir\"] = f\"/kaggle/output/{CFG[MODEL_NAME]['execution']['exp_id']}\"\n",
    "!rm -r {CFG[\"output_dir\"]}\n",
    "os.makedirs(CFG[\"output_dir\"], exist_ok=True)\n",
    "\n",
    "init_logger(f\"{ CFG[MODEL_NAME]['execution']['exp_id']}.log\")\n",
    "logger = get_logger(\"main\")\n",
    "seed_base_torch(CFG[MODEL_NAME][\"execution\"][\"seed\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "\n",
    "import numpy as np\n",
    "import polars as pl\n",
    "from tqdm import tqdm\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(45098, 33)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><style>\n",
       ".dataframe > thead > tr,\n",
       ".dataframe > tbody > tr {\n",
       "  text-align: right;\n",
       "  white-space: pre-wrap;\n",
       "}\n",
       "</style>\n",
       "<small>shape: (1, 33)</small><table border=\"1\" class=\"dataframe\"><thead><tr><th>ID</th><th>vEgo</th><th>aEgo</th><th>steeringAngleDeg</th><th>steeringTorque</th><th>brake</th><th>brakePressed</th><th>gas</th><th>gasPressed</th><th>gearShifter</th><th>leftBlinker</th><th>rightBlinker</th><th>x_0</th><th>y_0</th><th>z_0</th><th>x_1</th><th>y_1</th><th>z_1</th><th>x_2</th><th>y_2</th><th>z_2</th><th>x_3</th><th>y_3</th><th>z_3</th><th>x_4</th><th>y_4</th><th>z_4</th><th>x_5</th><th>y_5</th><th>z_5</th><th>sceneID</th><th>offset</th><th>submit</th></tr><tr><td>str</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>bool</td><td>f64</td><td>bool</td><td>str</td><td>bool</td><td>bool</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>str</td><td>f32</td><td>bool</td></tr></thead><tbody><tr><td>&quot;00066be8e20318869c38c66be46663…</td><td>5.701526</td><td>1.538456</td><td>-2.165777</td><td>-139.0</td><td>0.0</td><td>false</td><td>0.25</td><td>true</td><td>&quot;drive&quot;</td><td>false</td><td>false</td><td>2.82959</td><td>0.032226</td><td>0.045187</td><td>6.231999</td><td>0.065895</td><td>0.107974</td><td>9.785009</td><td>0.124972</td><td>0.203649</td><td>13.485472</td><td>0.163448</td><td>0.302818</td><td>17.574227</td><td>0.174289</td><td>0.406331</td><td>21.951269</td><td>0.199503</td><td>0.485079</td><td>&quot;00066be8e20318869c38c66be46663…</td><td>320.0</td><td>false</td></tr></tbody></table></div>"
      ],
      "text/plain": [
       "shape: (1, 33)\n",
       "┌──────────────┬──────────┬──────────┬──────────────┬───┬──────────┬─────────────┬────────┬────────┐\n",
       "│ ID           ┆ vEgo     ┆ aEgo     ┆ steeringAngl ┆ … ┆ z_5      ┆ sceneID     ┆ offset ┆ submit │\n",
       "│ ---          ┆ ---      ┆ ---      ┆ eDeg         ┆   ┆ ---      ┆ ---         ┆ ---    ┆ ---    │\n",
       "│ str          ┆ f64      ┆ f64      ┆ ---          ┆   ┆ f64      ┆ str         ┆ f32    ┆ bool   │\n",
       "│              ┆          ┆          ┆ f64          ┆   ┆          ┆             ┆        ┆        │\n",
       "╞══════════════╪══════════╪══════════╪══════════════╪═══╪══════════╪═════════════╪════════╪════════╡\n",
       "│ 00066be8e203 ┆ 5.701526 ┆ 1.538456 ┆ -2.165777    ┆ … ┆ 0.485079 ┆ 00066be8e20 ┆ 320.0  ┆ false  │\n",
       "│ 18869c38c66b ┆          ┆          ┆              ┆   ┆          ┆ 318869c38c6 ┆        ┆        │\n",
       "│ e46663…      ┆          ┆          ┆              ┆   ┆          ┆ 6be46663…   ┆        ┆        │\n",
       "└──────────────┴──────────┴──────────┴──────────────┴───┴──────────┴─────────────┴────────┴────────┘"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train = pl.read_csv(os.path.join(CFG[\"dataset\"][\"competition_dir\"], \"train_features.csv\"))\n",
    "train = (\n",
    "    train.with_columns(\n",
    "        pl.col(\"ID\").str.split_exact(\"_\", n=1).struct.rename_fields([\"sceneID\", \"offset\"]).alias(\"fields\")\n",
    "    )\n",
    "    .unnest(\"fields\")\n",
    "    .with_columns(\n",
    "        pl.col(\"offset\").cast(pl.Float32),\n",
    "        pl.lit(False).alias(\"submit\"),\n",
    "    )\n",
    ")\n",
    "test = pl.read_csv(os.path.join(CFG[\"dataset\"][\"competition_dir\"], \"test_features.csv\"))\n",
    "test = (\n",
    "    test.with_columns(\n",
    "        pl.col(\"ID\").str.split_exact(\"_\", n=1).struct.rename_fields([\"sceneID\", \"offset\"]).alias(\"fields\")\n",
    "    )\n",
    "    .unnest(\"fields\")\n",
    "    .with_columns(\n",
    "        pl.col(\"offset\").cast(pl.Float32),\n",
    "        pl.lit(True).alias(\"submit\"),\n",
    "    )\n",
    ")\n",
    "train = pl.concat([train, test], how=\"diagonal\")\n",
    "print(train.shape)\n",
    "train.head(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# normalize\n",
    "train = train.with_columns(\n",
    "    pl.col(\"vEgo\") / 30,\n",
    "    pl.col(\"aEgo\"),\n",
    "    pl.col(\"steeringAngleDeg\") / 400,\n",
    "    pl.col(\"steeringTorque\") / 600,\n",
    "    pl.col(\"offset\") / pl.col(\"offset\").max(),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "55\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><style>\n",
       ".dataframe > thead > tr,\n",
       ".dataframe > tbody > tr {\n",
       "  text-align: right;\n",
       "  white-space: pre-wrap;\n",
       "}\n",
       "</style>\n",
       "<small>shape: (9, 56)</small><table border=\"1\" class=\"dataframe\"><thead><tr><th>statistic</th><th>aEgo</th><th>aEgo_diff_-1</th><th>aEgo_diff_1</th><th>aEgo_shift_-1</th><th>aEgo_shift_1</th><th>brake</th><th>brakePressed</th><th>brakePressed_diff_-1</th><th>brakePressed_diff_1</th><th>brakePressed_shift_-1</th><th>brakePressed_shift_1</th><th>brake_diff_-1</th><th>brake_diff_1</th><th>brake_shift_-1</th><th>brake_shift_1</th><th>gas</th><th>gasPressed</th><th>gasPressed_diff_-1</th><th>gasPressed_diff_1</th><th>gasPressed_shift_-1</th><th>gasPressed_shift_1</th><th>gas_diff_-1</th><th>gas_diff_1</th><th>gas_shift_-1</th><th>gas_shift_1</th><th>leftBlinker</th><th>leftBlinker_diff_-1</th><th>leftBlinker_diff_1</th><th>leftBlinker_shift_-1</th><th>leftBlinker_shift_1</th><th>offset</th><th>offset_diff_-1</th><th>offset_diff_1</th><th>offset_shift_-1</th><th>offset_shift_1</th><th>rightBlinker</th><th>rightBlinker_diff_-1</th><th>rightBlinker_diff_1</th><th>rightBlinker_shift_-1</th><th>rightBlinker_shift_1</th><th>steeringAngleDeg</th><th>steeringAngleDeg_diff_-1</th><th>steeringAngleDeg_diff_1</th><th>steeringAngleDeg_shift_-1</th><th>steeringAngleDeg_shift_1</th><th>steeringTorque</th><th>steeringTorque_diff_-1</th><th>steeringTorque_diff_1</th><th>steeringTorque_shift_-1</th><th>steeringTorque_shift_1</th><th>vEgo</th><th>vEgo_diff_-1</th><th>vEgo_diff_1</th><th>vEgo_shift_-1</th><th>vEgo_shift_1</th></tr><tr><td>str</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td></tr></thead><tbody><tr><td>&quot;count&quot;</td><td>45098.0</td><td>35577.0</td><td>35577.0</td><td>35577.0</td><td>35577.0</td><td>45098.0</td><td>45098.0</td><td>35577.0</td><td>35577.0</td><td>35577.0</td><td>35577.0</td><td>35577.0</td><td>35577.0</td><td>35577.0</td><td>35577.0</td><td>45098.0</td><td>45098.0</td><td>35577.0</td><td>35577.0</td><td>35577.0</td><td>35577.0</td><td>35577.0</td><td>35577.0</td><td>35577.0</td><td>35577.0</td><td>45098.0</td><td>35577.0</td><td>35577.0</td><td>35577.0</td><td>35577.0</td><td>45098.0</td><td>35577.0</td><td>35577.0</td><td>35577.0</td><td>35577.0</td><td>45098.0</td><td>35577.0</td><td>35577.0</td><td>35577.0</td><td>35577.0</td><td>45098.0</td><td>35577.0</td><td>35577.0</td><td>35577.0</td><td>35577.0</td><td>45098.0</td><td>35577.0</td><td>35577.0</td><td>35577.0</td><td>35577.0</td><td>45098.0</td><td>35577.0</td><td>35577.0</td><td>35577.0</td><td>35577.0</td></tr><tr><td>&quot;null_count&quot;</td><td>0.0</td><td>9521.0</td><td>9521.0</td><td>9521.0</td><td>9521.0</td><td>0.0</td><td>0.0</td><td>9521.0</td><td>9521.0</td><td>9521.0</td><td>9521.0</td><td>9521.0</td><td>9521.0</td><td>9521.0</td><td>9521.0</td><td>0.0</td><td>0.0</td><td>9521.0</td><td>9521.0</td><td>9521.0</td><td>9521.0</td><td>9521.0</td><td>9521.0</td><td>9521.0</td><td>9521.0</td><td>0.0</td><td>9521.0</td><td>9521.0</td><td>9521.0</td><td>9521.0</td><td>0.0</td><td>9521.0</td><td>9521.0</td><td>9521.0</td><td>9521.0</td><td>0.0</td><td>9521.0</td><td>9521.0</td><td>9521.0</td><td>9521.0</td><td>0.0</td><td>9521.0</td><td>9521.0</td><td>9521.0</td><td>9521.0</td><td>0.0</td><td>9521.0</td><td>9521.0</td><td>9521.0</td><td>9521.0</td><td>0.0</td><td>9521.0</td><td>9521.0</td><td>9521.0</td><td>9521.0</td></tr><tr><td>&quot;mean&quot;</td><td>-0.016168</td><td>-0.005095</td><td>0.005095</td><td>-0.005095</td><td>0.005095</td><td>0.0</td><td>0.307131</td><td>0.001883</td><td>-0.001883</td><td>0.001883</td><td>-0.001883</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0.083971</td><td>0.483791</td><td>-0.001939</td><td>0.001939</td><td>-0.001939</td><td>0.001939</td><td>-0.000771</td><td>0.000771</td><td>-0.000771</td><td>0.000771</td><td>0.077764</td><td>-0.005228</td><td>0.005228</td><td>-0.005228</td><td>0.005228</td><td>0.515167</td><td>-0.206897</td><td>0.206897</td><td>-0.206897</td><td>0.206897</td><td>0.102067</td><td>-0.001068</td><td>0.001068</td><td>-0.001068</td><td>0.001068</td><td>-0.005274</td><td>0.000025</td><td>-0.000025</td><td>0.000025</td><td>-0.000025</td><td>-0.031763</td><td>-0.000984</td><td>0.000984</td><td>-0.000984</td><td>0.000984</td><td>0.306156</td><td>0.001376</td><td>-0.001376</td><td>0.001376</td><td>-0.001376</td></tr><tr><td>&quot;std&quot;</td><td>0.631639</td><td>0.860365</td><td>0.860365</td><td>0.860365</td><td>0.860365</td><td>0.0</td><td>0.461309</td><td>0.487097</td><td>0.487097</td><td>0.487097</td><td>0.487097</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0.109826</td><td>0.499743</td><td>0.564206</td><td>0.564206</td><td>0.564206</td><td>0.564206</td><td>0.126945</td><td>0.126945</td><td>0.126945</td><td>0.126945</td><td>0.267803</td><td>0.255094</td><td>0.255094</td><td>0.255094</td><td>0.255094</td><td>0.32926</td><td>0.075381</td><td>0.075381</td><td>0.075381</td><td>0.075381</td><td>0.302739</td><td>0.238165</td><td>0.238165</td><td>0.238165</td><td>0.238165</td><td>0.163457</td><td>0.186424</td><td>0.186424</td><td>0.186424</td><td>0.186424</td><td>0.184867</td><td>0.249753</td><td>0.249753</td><td>0.249753</td><td>0.249753</td><td>0.240915</td><td>0.086836</td><td>0.086836</td><td>0.086836</td><td>0.086836</td></tr><tr><td>&quot;min&quot;</td><td>-4.936206</td><td>-5.84588</td><td>-5.796605</td><td>-5.84588</td><td>-5.796605</td><td>0.0</td><td>0.0</td><td>-1.0</td><td>-1.0</td><td>-1.0</td><td>-1.0</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0.0</td><td>-1.0</td><td>-1.0</td><td>-1.0</td><td>-1.0</td><td>-0.8</td><td>-0.66</td><td>-0.8</td><td>-0.66</td><td>0.0</td><td>-1.0</td><td>-1.0</td><td>-1.0</td><td>-1.0</td><td>0.038462</td><td>-0.961538</td><td>0.192308</td><td>-0.961538</td><td>0.192308</td><td>0.0</td><td>-1.0</td><td>-1.0</td><td>-1.0</td><td>-1.0</td><td>-1.203486</td><td>-1.962355</td><td>-2.337468</td><td>-1.962355</td><td>-2.337468</td><td>-1.166667</td><td>-1.481667</td><td>-1.371667</td><td>-1.481667</td><td>-1.371667</td><td>-0.005569</td><td>-0.604864</td><td>-0.57511</td><td>-0.604864</td><td>-0.57511</td></tr><tr><td>&quot;25%&quot;</td><td>-0.237048</td><td>-0.323888</td><td>-0.368738</td><td>-0.323888</td><td>-0.368738</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0.0</td><td>-0.04</td><td>-0.05</td><td>-0.04</td><td>-0.05</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0.230769</td><td>-0.192308</td><td>0.192308</td><td>-0.192308</td><td>0.192308</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0.0</td><td>-0.008665</td><td>-0.008017</td><td>-0.007615</td><td>-0.008017</td><td>-0.007615</td><td>-0.156667</td><td>-0.146667</td><td>-0.143333</td><td>-0.146667</td><td>-0.143333</td><td>0.086478</td><td>-0.033128</td><td>-0.031659</td><td>-0.033128</td><td>-0.031659</td></tr><tr><td>&quot;50%&quot;</td><td>-2.1050e-15</td><td>-8.0168e-37</td><td>8.0168e-37</td><td>-8.0168e-37</td><td>8.0168e-37</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0.423077</td><td>-0.192308</td><td>0.192308</td><td>-0.192308</td><td>0.192308</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0.0</td><td>-0.0009</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0.0</td><td>-0.021667</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0.284483</td><td>-1.5867e-13</td><td>1.5867e-13</td><td>-1.5867e-13</td><td>1.5867e-13</td></tr><tr><td>&quot;75%&quot;</td><td>0.222053</td><td>0.368738</td><td>0.323888</td><td>0.368738</td><td>0.323888</td><td>0.0</td><td>1.0</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0.165</td><td>1.0</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0.05</td><td>0.04</td><td>0.05</td><td>0.04</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0.807692</td><td>-0.192308</td><td>0.192308</td><td>-0.192308</td><td>0.192308</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0.00653</td><td>0.007615</td><td>0.008017</td><td>0.007615</td><td>0.008017</td><td>0.085</td><td>0.143333</td><td>0.146667</td><td>0.143333</td><td>0.146667</td><td>0.47714</td><td>0.031659</td><td>0.033128</td><td>0.031659</td><td>0.033128</td></tr><tr><td>&quot;max&quot;</td><td>3.14007</td><td>5.796605</td><td>5.84588</td><td>5.796605</td><td>5.84588</td><td>0.0</td><td>1.0</td><td>1.0</td><td>1.0</td><td>1.0</td><td>1.0</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0.915</td><td>1.0</td><td>1.0</td><td>1.0</td><td>1.0</td><td>1.0</td><td>0.66</td><td>0.8</td><td>0.66</td><td>0.8</td><td>1.0</td><td>1.0</td><td>1.0</td><td>1.0</td><td>1.0</td><td>1.0</td><td>-0.192308</td><td>0.961538</td><td>-0.192308</td><td>0.961538</td><td>1.0</td><td>1.0</td><td>1.0</td><td>1.0</td><td>1.0</td><td>1.211729</td><td>2.337468</td><td>1.962355</td><td>2.337468</td><td>1.962355</td><td>1.166667</td><td>1.371667</td><td>1.481667</td><td>1.371667</td><td>1.481667</td><td>0.918375</td><td>0.57511</td><td>0.604864</td><td>0.57511</td><td>0.604864</td></tr></tbody></table></div>"
      ],
      "text/plain": [
       "shape: (9, 56)\n",
       "┌───────────┬───────────┬───────────┬───────────┬───┬───────────┬───────────┬───────────┬──────────┐\n",
       "│ statistic ┆ aEgo      ┆ aEgo_diff ┆ aEgo_diff ┆ … ┆ vEgo_diff ┆ vEgo_diff ┆ vEgo_shif ┆ vEgo_shi │\n",
       "│ ---       ┆ ---       ┆ _-1       ┆ _1        ┆   ┆ _-1       ┆ _1        ┆ t_-1      ┆ ft_1     │\n",
       "│ str       ┆ f64       ┆ ---       ┆ ---       ┆   ┆ ---       ┆ ---       ┆ ---       ┆ ---      │\n",
       "│           ┆           ┆ f64       ┆ f64       ┆   ┆ f64       ┆ f64       ┆ f64       ┆ f64      │\n",
       "╞═══════════╪═══════════╪═══════════╪═══════════╪═══╪═══════════╪═══════════╪═══════════╪══════════╡\n",
       "│ count     ┆ 45098.0   ┆ 35577.0   ┆ 35577.0   ┆ … ┆ 35577.0   ┆ 35577.0   ┆ 35577.0   ┆ 35577.0  │\n",
       "│ null_coun ┆ 0.0       ┆ 9521.0    ┆ 9521.0    ┆ … ┆ 9521.0    ┆ 9521.0    ┆ 9521.0    ┆ 9521.0   │\n",
       "│ t         ┆           ┆           ┆           ┆   ┆           ┆           ┆           ┆          │\n",
       "│ mean      ┆ -0.016168 ┆ -0.005095 ┆ 0.005095  ┆ … ┆ 0.001376  ┆ -0.001376 ┆ 0.001376  ┆ -0.00137 │\n",
       "│           ┆           ┆           ┆           ┆   ┆           ┆           ┆           ┆ 6        │\n",
       "│ std       ┆ 0.631639  ┆ 0.860365  ┆ 0.860365  ┆ … ┆ 0.086836  ┆ 0.086836  ┆ 0.086836  ┆ 0.086836 │\n",
       "│ min       ┆ -4.936206 ┆ -5.84588  ┆ -5.796605 ┆ … ┆ -0.604864 ┆ -0.57511  ┆ -0.604864 ┆ -0.57511 │\n",
       "│ 25%       ┆ -0.237048 ┆ -0.323888 ┆ -0.368738 ┆ … ┆ -0.033128 ┆ -0.031659 ┆ -0.033128 ┆ -0.03165 │\n",
       "│           ┆           ┆           ┆           ┆   ┆           ┆           ┆           ┆ 9        │\n",
       "│ 50%       ┆ -2.1050e- ┆ -8.0168e- ┆ 8.0168e-3 ┆ … ┆ -1.5867e- ┆ 1.5867e-1 ┆ -1.5867e- ┆ 1.5867e- │\n",
       "│           ┆ 15        ┆ 37        ┆ 7         ┆   ┆ 13        ┆ 3         ┆ 13        ┆ 13       │\n",
       "│ 75%       ┆ 0.222053  ┆ 0.368738  ┆ 0.323888  ┆ … ┆ 0.031659  ┆ 0.033128  ┆ 0.031659  ┆ 0.033128 │\n",
       "│ max       ┆ 3.14007   ┆ 5.796605  ┆ 5.84588   ┆ … ┆ 0.57511   ┆ 0.604864  ┆ 0.57511   ┆ 0.604864 │\n",
       "└───────────┴───────────┴───────────┴───────────┴───┴───────────┴───────────┴───────────┴──────────┘"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from penguinml.utils.contena import FeatureContena\n",
    "\n",
    "from feature_engineering import add_basic_features, add_scene_lag_features\n",
    "\n",
    "features = FeatureContena()\n",
    "train, features = add_basic_features(train, features)\n",
    "train, features = add_scene_lag_features(train, features)\n",
    "print(len(features.num_features()))\n",
    "train[features.num_features()].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fillna\n",
    "for col in features.num_features():\n",
    "    train = train.with_columns(pl.col(col).fill_null(-1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div><style>\n",
       ".dataframe > thead > tr,\n",
       ".dataframe > tbody > tr {\n",
       "  text-align: right;\n",
       "  white-space: pre-wrap;\n",
       "}\n",
       "</style>\n",
       "<small>shape: (1, 2)</small><table border=\"1\" class=\"dataframe\"><thead><tr><th>ID</th><th>Description</th></tr><tr><td>str</td><td>str</td></tr></thead><tbody><tr><td>&quot;00066be8e20318869c38c66be46663…</td><td>&quot;The center and right portions …</td></tr></tbody></table></div>"
      ],
      "text/plain": [
       "shape: (1, 2)\n",
       "┌─────────────────────────────────┬─────────────────────────────────┐\n",
       "│ ID                              ┆ Description                     │\n",
       "│ ---                             ┆ ---                             │\n",
       "│ str                             ┆ str                             │\n",
       "╞═════════════════════════════════╪═════════════════════════════════╡\n",
       "│ 00066be8e20318869c38c66be46663… ┆ The center and right portions … │\n",
       "└─────────────────────────────────┴─────────────────────────────────┘"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# vlm\n",
    "sentence_df = pl.read_csv(\"/kaggle/input/vlm_sentence/future_position_descriptions.csv\")\n",
    "train = train.join(sentence_df, on=\"ID\", how=\"left\")\n",
    "sentence_df.head(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pytorch_lightning as lightning\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torchvision\n",
    "from pytorch_lightning.callbacks import EarlyStopping, ModelCheckpoint\n",
    "from torch.autograd import Variable\n",
    "from torch.nn import functional as F\n",
    "from torch.optim import lr_scheduler\n",
    "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
    "from torchmetrics import (\n",
    "    AUROC,\n",
    "    Accuracy,\n",
    "    F1Score,\n",
    "    MeanAbsoluteError,\n",
    "    MeanSquaredError,\n",
    "    MetricCollection,\n",
    "    Precision,\n",
    "    Recall,\n",
    ")\n",
    "from torchvision import datasets, models, transforms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'token_ids': tensor([    0,   133,  1312,     8,   235, 14566,     9,     5,  2274,  6364,\n",
       "             5,  2621,     9,   921, 35480,     6,   217,  7993,     6,    10,\n",
       "          9128,     6,     8,    10, 11038,  1656,  1970,    15,     5,   314,\n",
       "           526,     4,    20,  1881,     9,     5,  1703,   797,  4666,    16,\n",
       "           699,     6, 31843,     6,  5542,     5,  1155,    16,    15,    41,\n",
       "          2171,  2014,     4,   374,     5,   235,   526,     6,     5,   921,\n",
       "          2092,     7,    28,  2342,  7933,     6,  3544, 22206,     9,    10,\n",
       "          2353,   906,     6,  3228,    12, 21765,   921,    19,    97,  1734,\n",
       "          1455,     4, 50118, 50118, 20930,    15,     5,   699, 35480,     8,\n",
       "             5,  1703,  8724,     6,     5,  1155,    18,   595,  2698,    16,\n",
       "          1706,     5,  9128,    15,     5,   314,   526,     4,   318,     5,\n",
       "          1155,  1388,    31,     5,   595,  6625,     6,    24,    74,   533,\n",
       "          1004,     7,     5,   235,    25,     5,  1703,  8724,  6364,    24,\n",
       "            18,  1522,     7,  9073,    88,    10, 11038,    12, 13268,  6625,\n",
       "             4,     2,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "             1,     1,     1,     1]),\n",
       " 'meta': tensor([ 1.5385,  1.2586, -1.0000,  1.2586, -1.0000,  0.0000,  0.0000,  0.0000,\n",
       "         -1.0000,  0.0000, -1.0000,  0.0000, -1.0000,  0.0000, -1.0000,  0.2500,\n",
       "          1.0000,  1.0000, -1.0000,  1.0000, -1.0000,  0.2500, -1.0000,  0.2500,\n",
       "         -1.0000,  0.0000,  0.0000, -1.0000,  0.0000, -1.0000,  0.6154, -0.1923,\n",
       "         -1.0000, -0.1923, -1.0000,  0.0000, -1.0000, -1.0000, -1.0000, -1.0000,\n",
       "         -0.0054,  0.0236, -1.0000,  0.0236, -1.0000, -0.2317, -0.1583, -1.0000,\n",
       "         -0.1583, -1.0000,  0.1901, -0.1825, -1.0000, -0.1825, -1.0000]),\n",
       " 'token_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]),\n",
       " 'target': array([ 2.8295903 ,  0.03222637,  0.04518677,  6.2319994 ,  0.06589481,\n",
       "         0.10797438,  9.785009  ,  0.12497172,  0.20364925, 13.485473  ,\n",
       "         0.16344823,  0.30281773, 17.574226  ,  0.17428894,  0.4063311 ,\n",
       "        21.95127   ,  0.19950305,  0.4850789 ], dtype=float32)}"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import AutoModel, AutoTokenizer\n",
    "\n",
    "from const import TARGET_COLS\n",
    "\n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\n",
    "\n",
    "\n",
    "BERT_MODEL_NAME = \"roberta-base\"\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(BERT_MODEL_NAME)\n",
    "\n",
    "\n",
    "class MyDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, df: pl.DataFrame, tokenizer):\n",
    "        self.X_token = tokenizer(\n",
    "            df[\"Description\"].to_list(),\n",
    "            padding=True,\n",
    "            truncation=True,\n",
    "            return_tensors=\"pt\",\n",
    "        )\n",
    "        self.meta_features = df[features.num_features()].to_numpy().astype(np.float32)\n",
    "        self.y = df[TARGET_COLS].to_numpy().astype(np.float32)\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.y.shape[0]\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        return {\n",
    "            \"token_ids\": torch.LongTensor(self.X_token[\"input_ids\"][index]),\n",
    "            \"meta\": torch.FloatTensor(self.meta_features[index]),\n",
    "            \"token_mask\": torch.LongTensor(self.X_token[\"attention_mask\"][index]),\n",
    "            \"target\": self.y[index],\n",
    "        }\n",
    "\n",
    "\n",
    "class LightningDataModule(lightning.LightningDataModule):\n",
    "    def __init__(self, train_dataset=None, valid_dataset=None, batch_size=32):\n",
    "        super().__init__()\n",
    "        self.train_dataset = train_dataset\n",
    "        self.valid_dataset = valid_dataset\n",
    "        self.batch_size = batch_size\n",
    "\n",
    "    def train_dataloader(self):\n",
    "        return torch.utils.data.DataLoader(\n",
    "            self.train_dataset,\n",
    "            batch_size=self.batch_size,\n",
    "            shuffle=True,\n",
    "            pin_memory=True,\n",
    "            worker_init_fn=self.worker_init_fn,\n",
    "            num_workers=2,\n",
    "        )\n",
    "\n",
    "    def val_dataloader(self):\n",
    "        return torch.utils.data.DataLoader(\n",
    "            self.valid_dataset,\n",
    "            batch_size=self.batch_size,\n",
    "            shuffle=False,\n",
    "            pin_memory=True,\n",
    "            worker_init_fn=self.worker_init_fn,\n",
    "            num_workers=2,\n",
    "        )\n",
    "\n",
    "    def worker_init_fn(self, worker_id):\n",
    "        # dataloaderでnum_workers>1の時の乱数設定\n",
    "        # これを指定しないと各workerのrandom_stateが同じになり、データも同じになる。\n",
    "        np.random.seed(np.random.get_state()[1][0] + worker_id)\n",
    "\n",
    "\n",
    "MyDataset(train.head(10), tokenizer)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gc\n",
    "\n",
    "\n",
    "class fc_bn_relu(nn.Module):\n",
    "    def __init__(self, d_in, d_out):\n",
    "        super(fc_bn_relu, self).__init__()\n",
    "        self.fc = nn.Linear(d_in, d_out)\n",
    "        self.bn = nn.BatchNorm1d(d_out)\n",
    "        self.relu = nn.ReLU()\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.fc(x)\n",
    "        x = self.bn(x)\n",
    "        x = self.relu(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "class LightningModel(lightning.LightningModule):\n",
    "    def __init__(\n",
    "        self,\n",
    "        loss_fn,\n",
    "        lr=0.001,\n",
    "        weight_decay=0,\n",
    "    ):\n",
    "        super().__init__()\n",
    "\n",
    "        self.sentence_encoder = AutoModel.from_pretrained(BERT_MODEL_NAME)\n",
    "        self.bert_fc = torch.nn.Linear(self.sentence_encoder.config.hidden_size, len(TARGET_COLS))\n",
    "\n",
    "        self.loss_fn = loss_fn\n",
    "        self.lr = lr\n",
    "        self.weight_decay = weight_decay\n",
    "\n",
    "        self.train_metrics = MetricCollection([MeanAbsoluteError()], prefix=\"\")\n",
    "        self.valid_metrics = MetricCollection([MeanAbsoluteError()], prefix=\"val_\")\n",
    "\n",
    "        self.val_step_outputs = []\n",
    "        self.val_step_labels = []\n",
    "\n",
    "    def forward(self, x_token_ids, x_mask):\n",
    "        x = self.sentence_encoder(x_token_ids, attention_mask=x_mask)[1]\n",
    "        x = self.bert_fc(x)\n",
    "        # x = x.squeeze(1)\n",
    "        return x\n",
    "\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        x_token_ids = batch[\"token_ids\"]\n",
    "        x_mask = batch[\"token_mask\"]\n",
    "        y = batch[\"target\"]\n",
    "        preds = self.forward(x_token_ids, x_mask)\n",
    "        loss = self.loss_fn(preds, y)\n",
    "\n",
    "        # self.train_metrics(preds, y.to(int))\n",
    "        self.log(\n",
    "            \"loss\",\n",
    "            loss,\n",
    "            prog_bar=True,\n",
    "            logger=True,\n",
    "            on_epoch=True,\n",
    "            on_step=True,\n",
    "        )\n",
    "        self.log_dict(\n",
    "            self.train_metrics,\n",
    "            prog_bar=True,\n",
    "            logger=True,\n",
    "            on_epoch=True,\n",
    "            on_step=True,\n",
    "        )\n",
    "        return {\"loss\": loss}\n",
    "\n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        x_token_ids = batch[\"token_ids\"]\n",
    "        x_mask = batch[\"token_mask\"]\n",
    "        y = batch[\"target\"]\n",
    "        preds = self.forward(x_token_ids, x_mask)\n",
    "\n",
    "        self.val_step_outputs.append(preds)\n",
    "        self.val_step_labels.append(y)\n",
    "\n",
    "    def on_validation_epoch_end(self):\n",
    "        preds = torch.cat(self.val_step_outputs)\n",
    "        labels = torch.cat(self.val_step_labels)\n",
    "        self.val_step_outputs.clear()\n",
    "        self.val_step_labels.clear()\n",
    "        gc.collect()\n",
    "        loss = self.loss_fn(preds, labels)\n",
    "\n",
    "        print(f\"[epoch {self.trainer.current_epoch}]\")\n",
    "\n",
    "        self.log(\n",
    "            \"val_loss\",\n",
    "            loss,\n",
    "            prog_bar=False,\n",
    "            logger=True,\n",
    "            on_epoch=True,\n",
    "            on_step=False,\n",
    "        )\n",
    "        self.log_dict(\n",
    "            self.valid_metrics,\n",
    "            prog_bar=True,\n",
    "            logger=True,\n",
    "            on_epoch=True,\n",
    "            on_step=False,\n",
    "        )\n",
    "\n",
    "        # ログをprint\n",
    "        self.print_metric(preds, labels, \"valid\")\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        optimizer = optim.Adam(self.parameters(), lr=self.lr, weight_decay=self.weight_decay)\n",
    "        scheduler = ReduceLROnPlateau(optimizer, factor=0.1, patience=2, verbose=True)\n",
    "        # scheduler = LinearWarmupCosineAnnealingLR(optimizer, eta_min=1e-8, max_epochs=1e10,\n",
    "        #                                           warmup_epochs=10, warmup_start_lr=1e-8)\n",
    "        return {\"optimizer\": optimizer, \"lr_scheduler\": scheduler, \"monitor\": \"val_loss\"}\n",
    "\n",
    "    def print_metric(self, y_hat, y, train_or_valid=\"train\"):\n",
    "        \"\"\"\n",
    "        ログをprintする。次のepochが終わると上書きされてしまうので。\n",
    "        TODO: たぶんもっとマシな方法があるので探す。\n",
    "        \"\"\"\n",
    "        if train_or_valid == \"train\":\n",
    "            metrics = self.train_metrics\n",
    "        else:\n",
    "            metrics = self.valid_metrics\n",
    "        loss = self.loss_fn(y_hat, y)\n",
    "\n",
    "        print(f\"[epoch {self.trainer.current_epoch}] {train_or_valid}: \", end=\"\")\n",
    "        print(f\"{type(self.loss_fn).__name__}={loss:.4f}\", end=\", \")\n",
    "        for name, metric in metrics.items():\n",
    "            v = metric(y_hat, y.to(int))\n",
    "            print(f\"{name}={v:.4f}\", end=\", \")\n",
    "        print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CV Split\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_folds = pl.read_csv(CFG[\"dataset\"][\"train_fold_path\"])\n",
    "train = train.join(train_folds, on=\"sceneID\", how=\"left\")\n",
    "# assert train[\"fold\"].null_count() == 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from penguinml.nn.utils.dataloader import get_dataloaders\n",
    "from penguinml.nn.utils.trainer import Trainer\n",
    "from transformers import get_cosine_schedule_with_warmup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rm: cannot remove 'lightning_logs': No such file or directory\n",
      "rm: cannot remove 'logs': No such file or directory\n"
     ]
    }
   ],
   "source": [
    "!rm -r lightning_logs\n",
    "!rm -r logs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3.6760581381351227"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# naive\n",
    "scores = []\n",
    "for c in TARGET_COLS:\n",
    "    mae = (train[c] - train[c].median()).abs().mean()\n",
    "    scores.append(mae)\n",
    "np.mean(scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-base and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "289232cc080a4bc29367e4ef244744eb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f16887b2c220463b9eb65c8286b4ce9e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[epoch 0]\n",
      "[epoch 0] valid: L1Loss=3.5935, val_MeanAbsoluteError=3.4535, \n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1892ddae98554ed3813024af8f0eeada",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[epoch 1]\n",
      "[epoch 1] valid: L1Loss=3.2286, val_MeanAbsoluteError=3.1046, \n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9c2ba617a49b4e658c343746a3b75ba5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[epoch 2]\n",
      "[epoch 2] valid: L1Loss=3.0849, val_MeanAbsoluteError=2.9595, \n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "feeba9a1f2f0449780ba1b05eaf09f2b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[epoch 3]\n",
      "[epoch 3] valid: L1Loss=3.1180, val_MeanAbsoluteError=2.9649, \n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "256356e7b58d4d00803e9a78c536cd39",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[epoch 4]\n",
      "[epoch 4] valid: L1Loss=3.0404, val_MeanAbsoluteError=2.9120, \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-base and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "100%|██████████| 362/362 [00:35<00:00, 10.12it/s]\n",
      "100%|██████████| 72/72 [00:07<00:00, 10.18it/s]\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-base and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "040431bf358c410fb59183ee3ad914b9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9ea5420f71a94f58b741786726431fd5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[epoch 0]\n",
      "[epoch 0] valid: L1Loss=3.5827, val_MeanAbsoluteError=3.4509, \n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "db2555ecf601473d99bbdde64b386e87",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[epoch 1]\n",
      "[epoch 1] valid: L1Loss=3.1997, val_MeanAbsoluteError=3.0691, \n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c5387c505f994e1bb4a07365ced23c81",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[epoch 2]\n",
      "[epoch 2] valid: L1Loss=3.1050, val_MeanAbsoluteError=2.9974, \n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0c4d0b4448a44fb6b3e6c6b3b524ddf2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[epoch 3]\n",
      "[epoch 3] valid: L1Loss=3.0495, val_MeanAbsoluteError=2.9304, \n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ea07eee2b11141c7a54c4b062faa4020",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[epoch 4]\n",
      "[epoch 4] valid: L1Loss=3.0415, val_MeanAbsoluteError=2.9273, \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-base and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "100%|██████████| 362/362 [00:35<00:00, 10.23it/s]\n",
      "100%|██████████| 72/72 [00:07<00:00, 10.25it/s]\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-base and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "16f475dd6b6542e2a91ff35a014be461",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a0befb4a408c41c29694f1459e2b2706",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[epoch 0]\n",
      "[epoch 0] valid: L1Loss=3.6579, val_MeanAbsoluteError=3.5125, \n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b4a09554d42349cfa39f62d28ea75c5f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[epoch 1]\n",
      "[epoch 1] valid: L1Loss=3.2648, val_MeanAbsoluteError=3.1247, \n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "700c000e4dcf40ebb3c9de011472d6ae",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[epoch 2]\n",
      "[epoch 2] valid: L1Loss=3.1533, val_MeanAbsoluteError=3.0367, \n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "02bc47d7ba0f441389f445935ea3c55d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[epoch 3]\n",
      "[epoch 3] valid: L1Loss=3.0877, val_MeanAbsoluteError=2.9692, \n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f07acff515834ba7b5ac7d54e7f40bc9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[epoch 4]\n",
      "[epoch 4] valid: L1Loss=3.0957, val_MeanAbsoluteError=2.9563, \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-base and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "100%|██████████| 362/362 [00:35<00:00, 10.20it/s]\n",
      "100%|██████████| 72/72 [00:07<00:00, 10.25it/s]\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-base and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "316bde20b98d4403838849ce04573bf3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2e511e08506d41ada86d13892746482c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[epoch 0]\n",
      "[epoch 0] valid: L1Loss=3.7819, val_MeanAbsoluteError=3.6445, \n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "270be64740384077a3262cbb6e6f913d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[epoch 1]\n",
      "[epoch 1] valid: L1Loss=3.3697, val_MeanAbsoluteError=3.2310, \n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e515756160794f4db5b71236f6c99694",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[epoch 2]\n",
      "[epoch 2] valid: L1Loss=3.1875, val_MeanAbsoluteError=3.0556, \n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6e103a0dc11541ab82de6461f474a459",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[epoch 3]\n",
      "[epoch 3] valid: L1Loss=3.0692, val_MeanAbsoluteError=2.9482, \n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "db6b8980811f4e398627cf9b64191d3f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[epoch 4]\n",
      "[epoch 4] valid: L1Loss=3.0743, val_MeanAbsoluteError=2.9610, \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-base and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "100%|██████████| 362/362 [00:35<00:00, 10.14it/s]\n",
      "100%|██████████| 72/72 [00:07<00:00, 10.20it/s]\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-base and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b774c219c4c041dbb1a94a9787a1bf87",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "90d9dcf915874822adb62a8e0061cc79",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[epoch 0]\n",
      "[epoch 0] valid: L1Loss=3.5429, val_MeanAbsoluteError=3.4076, \n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "775e164310b84684a59f154c79c88672",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[epoch 1]\n",
      "[epoch 1] valid: L1Loss=3.2079, val_MeanAbsoluteError=3.0895, \n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "29884be0f815491aa6affcfc551c9d9a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[epoch 2]\n",
      "[epoch 2] valid: L1Loss=3.0523, val_MeanAbsoluteError=2.9289, \n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "97ee7759d8a04e97a7b2e0336c1ea488",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[epoch 3]\n",
      "[epoch 3] valid: L1Loss=3.0312, val_MeanAbsoluteError=2.8982, \n"
     ]
    }
   ],
   "source": [
    "oof_dfs = []\n",
    "sub_preds = []\n",
    "for fold in range(5):\n",
    "    train_df = train.filter(pl.col(\"fold\") != fold).filter(~pl.col(\"submit\"))\n",
    "    valid_df = train.filter(pl.col(\"fold\") == fold).filter(~pl.col(\"submit\"))\n",
    "    test_df = train.filter(pl.col(\"submit\")).sort(\"ID\")\n",
    "\n",
    "    train_dataset = MyDataset(train_df, tokenizer)\n",
    "    valid_dataset = MyDataset(valid_df, tokenizer)\n",
    "    test_dataset = MyDataset(test_df, tokenizer)\n",
    "\n",
    "    bs = 24\n",
    "    data_module = LightningDataModule(train_dataset, valid_dataset, batch_size=bs)\n",
    "    model = LightningModel(lr=0.00001, weight_decay=0, loss_fn=nn.L1Loss())\n",
    "\n",
    "    # コールバック\n",
    "    cp_callback = ModelCheckpoint(\n",
    "        \"logs/\",\n",
    "        filename=\"best_model\",\n",
    "        monitor=\"val_MeanAbsoluteError\",\n",
    "        mode=\"min\",\n",
    "        save_top_k=1,\n",
    "        save_last=False,\n",
    "    )\n",
    "    es_callback = EarlyStopping(\n",
    "        monitor=\"val_MeanAbsoluteError\",\n",
    "        mode=\"min\",\n",
    "        patience=3,\n",
    "    )\n",
    "\n",
    "    # 学習\n",
    "    trainer = lightning.Trainer(\n",
    "        callbacks=[cp_callback, es_callback],\n",
    "        max_epochs=5,\n",
    "        num_sanity_val_steps=0,\n",
    "    )\n",
    "    trainer.fit(model, datamodule=data_module)\n",
    "\n",
    "    # 推論\n",
    "    model = LightningModel.load_from_checkpoint(\"logs/best_model.ckpt\", loss_fn=nn.L1Loss()).to(\"cuda\")\n",
    "    pred = []\n",
    "    for i in tqdm(range(0, len(valid_dataset), bs)):\n",
    "        pred.append(\n",
    "            nn.Sigmoid()(\n",
    "                model(\n",
    "                    torch.stack([valid_dataset[i][\"token_ids\"] for i in range(i, min(i + bs, len(valid_dataset)))]).to(\n",
    "                        \"cuda\"\n",
    "                    ),\n",
    "                    torch.stack(\n",
    "                        [valid_dataset[i][\"token_mask\"] for i in range(i, min(i + bs, len(valid_dataset)))]\n",
    "                    ).to(\"cuda\"),\n",
    "                ).detach()\n",
    "            )\n",
    "            .cpu()\n",
    "            .numpy()\n",
    "        )\n",
    "    oof = np.concatenate(pred)\n",
    "    valid_df = valid_df.with_columns(*[pl.Series(TARGET_COLS[i], oof[:, i]) for i in range(len(TARGET_COLS))])\n",
    "    oof_dfs.append(valid_df.select([\"ID\"] + TARGET_COLS))\n",
    "\n",
    "    # test\n",
    "    pred = []\n",
    "    for i in tqdm(range(0, len(test_dataset), bs)):\n",
    "        pred.append(\n",
    "            nn.Sigmoid()(\n",
    "                model(\n",
    "                    torch.stack([test_dataset[i][\"token_ids\"] for i in range(i, min(i + bs, len(test_dataset)))]).to(\n",
    "                        \"cuda\"\n",
    "                    ),\n",
    "                    torch.stack([test_dataset[i][\"token_mask\"] for i in range(i, min(i + bs, len(test_dataset)))]).to(\n",
    "                        \"cuda\"\n",
    "                    ),\n",
    "                ).detach()\n",
    "            )\n",
    "            .cpu()\n",
    "            .numpy()\n",
    "        )\n",
    "    sub = np.concatenate(pred)\n",
    "    sub_preds.append(sub)\n",
    "\n",
    "    del model, trainer\n",
    "    gc.collect()\n",
    "    torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "operands could not be broadcast together with shapes (50,18) (43371,18) ",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[17], line 4\u001b[0m\n\u001b[1;32m      1\u001b[0m oof_df \u001b[38;5;241m=\u001b[39m pl\u001b[38;5;241m.\u001b[39mconcat(oof_dfs)\u001b[38;5;241m.\u001b[39msort(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mID\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m      2\u001b[0m train \u001b[38;5;241m=\u001b[39m train\u001b[38;5;241m.\u001b[39msort(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mID\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m----> 4\u001b[0m mae \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mmean(np\u001b[38;5;241m.\u001b[39mabs(\u001b[43moof_df\u001b[49m\u001b[43m[\u001b[49m\u001b[43mTARGET_COLS\u001b[49m\u001b[43m]\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto_numpy\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mtrain\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfilter\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m~\u001b[39;49m\u001b[43mpl\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcol\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43msubmit\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m[\u001b[49m\u001b[43mTARGET_COLS\u001b[49m\u001b[43m]\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto_numpy\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m))\n\u001b[1;32m      5\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mMAE: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmae\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m      7\u001b[0m oof_df\u001b[38;5;241m.\u001b[39mwrite_csv(os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(CFG[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124moutput_dir\u001b[39m\u001b[38;5;124m\"\u001b[39m], \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124moof.csv\u001b[39m\u001b[38;5;124m\"\u001b[39m))\n",
      "\u001b[0;31mValueError\u001b[0m: operands could not be broadcast together with shapes (50,18) (43371,18) "
     ]
    }
   ],
   "source": [
    "oof_df = pl.concat(oof_dfs).sort(\"ID\")\n",
    "train = train.sort(\"ID\")\n",
    "\n",
    "mae = np.mean(np.abs(oof_df[TARGET_COLS].to_numpy() - train.filter(~pl.col(\"submit\"))[TARGET_COLS].to_numpy()))\n",
    "print(f\"MAE: {mae}\")\n",
    "\n",
    "oof_df.write_csv(os.path.join(CFG[\"output_dir\"], \"oof.csv\"))\n",
    "oof_df.head(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "too many indices for array: array is 1-dimensional, but 2 were indexed",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[19], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m sub_preds \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mstack(sub_preds)\u001b[38;5;241m.\u001b[39mmean(axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m)  \u001b[38;5;66;03m# medがいいかも\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m sub_df \u001b[38;5;241m=\u001b[39m test_df\u001b[38;5;241m.\u001b[39mwith_columns(\u001b[38;5;241m*\u001b[39m[pl\u001b[38;5;241m.\u001b[39mSeries(TARGET_COLS[i], sub_preds[:, i]) \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mlen\u001b[39m(TARGET_COLS))])\u001b[38;5;241m.\u001b[39msort(\n\u001b[1;32m      3\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mID\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m      4\u001b[0m )\n\u001b[1;32m      6\u001b[0m sub_df\u001b[38;5;241m.\u001b[39mselect([\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mID\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m+\u001b[39m TARGET_COLS)\u001b[38;5;241m.\u001b[39mwrite_csv(os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(CFG[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124moutput_dir\u001b[39m\u001b[38;5;124m\"\u001b[39m], \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msubmission.csv\u001b[39m\u001b[38;5;124m\"\u001b[39m))\n\u001b[1;32m      7\u001b[0m sub_df\u001b[38;5;241m.\u001b[39mhead(\u001b[38;5;241m1\u001b[39m)\n",
      "Cell \u001b[0;32mIn[19], line 2\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m      1\u001b[0m sub_preds \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mstack(sub_preds)\u001b[38;5;241m.\u001b[39mmean(axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m)  \u001b[38;5;66;03m# medがいいかも\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m sub_df \u001b[38;5;241m=\u001b[39m test_df\u001b[38;5;241m.\u001b[39mwith_columns(\u001b[38;5;241m*\u001b[39m[pl\u001b[38;5;241m.\u001b[39mSeries(TARGET_COLS[i], \u001b[43msub_preds\u001b[49m\u001b[43m[\u001b[49m\u001b[43m:\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mi\u001b[49m\u001b[43m]\u001b[49m) \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mlen\u001b[39m(TARGET_COLS))])\u001b[38;5;241m.\u001b[39msort(\n\u001b[1;32m      3\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mID\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m      4\u001b[0m )\n\u001b[1;32m      6\u001b[0m sub_df\u001b[38;5;241m.\u001b[39mselect([\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mID\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m+\u001b[39m TARGET_COLS)\u001b[38;5;241m.\u001b[39mwrite_csv(os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(CFG[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124moutput_dir\u001b[39m\u001b[38;5;124m\"\u001b[39m], \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msubmission.csv\u001b[39m\u001b[38;5;124m\"\u001b[39m))\n\u001b[1;32m      7\u001b[0m sub_df\u001b[38;5;241m.\u001b[39mhead(\u001b[38;5;241m1\u001b[39m)\n",
      "\u001b[0;31mIndexError\u001b[0m: too many indices for array: array is 1-dimensional, but 2 were indexed"
     ]
    }
   ],
   "source": [
    "sub_preds = np.stack(sub_preds).mean(axis=0)  # medがいいかも\n",
    "sub_df = (\n",
    "    test_df.with_columns(*[pl.Series(TARGET_COLS[i], sub_preds[:, i]) for i in range(len(TARGET_COLS))])\n",
    "    .sort(\"ID\")\n",
    "    .select([\"ID\"] + TARGET_COLS)\n",
    ")\n",
    "\n",
    "sub_df.write_csv(os.path.join(CFG[\"output_dir\"], \"submission.csv\"))\n",
    "sub_df.head(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
